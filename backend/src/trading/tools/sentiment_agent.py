"""Sentiment Agent - å¸‚å ´æƒ…ç·’åˆ†æè‡ªä¸»å‹ Agent

é€™å€‹æ¨¡çµ„å¯¦ä½œå…·æœ‰è‡ªä¸»åˆ†æèƒ½åŠ›çš„å¸‚å ´æƒ…ç·’åˆ†æ Agentã€‚
"""

from __future__ import annotations

import os
import json
from typing import Any
from datetime import datetime

from dotenv import load_dotenv
from pydantic import BaseModel

from agents import Agent, function_tool, ModelSettings
from agents.extensions.models.litellm_model import LitellmModel

from common.logger import logger

load_dotenv()

DEFAULT_MODEL = os.getenv("DEFAULT_AI_MODEL", "gpt-5-mini")
DEFAULT_MAX_TURNS = os.getenv("DEFAULT_MAX_TURNS", 30)

# ==========================================
# å…¨å±€ MCP ä¼ºæœå™¨ä¸Šä¸‹æ–‡
# ==========================================

# ç”¨æ–¼å­˜å„² async å·¥å…·å¯ä»¥è¨ªå•çš„ MCP ä¼ºæœå™¨å¯¦ä¾‹
_sentiment_agent_context = {
    "tavily_mcp": None,
}


# ==========================================
# åƒæ•¸é©—è­‰å’Œå®¹éŒ¯ Helper å‡½æ•¸
# ==========================================


def parse_tool_params(
    **kwargs,
) -> dict[str, Any]:
    """
    è§£æå’Œé©—è­‰ AI Agent å‚³å…¥çš„åƒæ•¸ã€‚

    è™•ç†å¤šç¨®æƒ…æ³ï¼š
    1. ç›´æ¥çš„åƒæ•¸ï¼šsymbol="2330", quantity=1000
    2. JSON å­—ä¸²åƒæ•¸ï¼šargs='{"symbol":"2330","quantity":1000}'
    3. å–®å€‹ 'input' åƒæ•¸ï¼ˆæŸäº› sub-agent å‘¼å«æ–¹å¼ï¼‰

    Args:
        **kwargs: å‚³å…¥çš„æ‰€æœ‰åƒæ•¸

    Returns:
        è§£æå¾Œçš„åƒæ•¸å­—å…¸
    """
    # å˜—è©¦å¾ 'args' åƒæ•¸ä¸­è§£æ JSON
    if "args" in kwargs and isinstance(kwargs["args"], str):
        try:
            parsed = json.loads(kwargs["args"])
            logger.debug(f"æˆåŠŸå¾ JSON å­—ä¸²è§£æåƒæ•¸: {parsed}")
            return parsed
        except json.JSONDecodeError:
            logger.debug(f"ç„¡æ³•è§£æ args ä¸­çš„ JSON: {kwargs['args']}")

    # ç§»é™¤ç„¡æ•ˆçš„åƒæ•¸ï¼ˆä¾‹å¦‚ input_imageï¼‰
    result = {}
    for k, v in kwargs.items():
        if k not in ["args", "input", "input_image"]:
            result[k] = v

    return result


# ==========================================
# MCP å·¥å…·å‘¼å«è¼”åŠ©å‡½æ•¸
# ==========================================


def _parse_detailed_results(text_content: str) -> list[dict[str, Any]]:
    """
    è§£æ Tavily è¿”å›çš„ 'Detailed Results' ç´”æ–‡æœ¬æ ¼å¼ã€‚

    æ ¼å¼ç¤ºä¾‹ï¼š
    Detailed Results:

    Title: [æ¨™çš„] 2881å¯Œé‚¦é‡‘èªªå¥½çš„æ•™è¨“å‘¢QQ? - çœ‹æ¿Stock - PTTç¶²é ç‰ˆ
    URL: https://www.pttweb.cc/bbs/Stock/M.1652840005.A.CB4
    Content: æ¨™çš„ï¼š2881.TW å¯Œé‚¦é‡‘2. åˆ†é¡ï¼šè¨è«–3. åˆ†æ/æ­£æ–‡ï¼š é€™å¹¾å¤©å¤§å®¶ä¸€ç›´ä¿å–®å•é¡Œ...

    Args:
        text_content: Tavily è¿”å›çš„ç´”æ–‡æœ¬å…§å®¹

    Returns:
        æœå°‹çµæœåˆ—è¡¨ï¼Œæ¯ç­†åŒ…å« title, url, content, source, timestamp
    """
    results = []

    try:
        # ç§»é™¤ "Detailed Results:" é ­éƒ¨
        content = text_content
        if "Detailed Results:" in content:
            content = content.split("Detailed Results:", 1)[1]

        # æŒ‰ "Title:" åˆ†å‰²çµæœ
        result_blocks = content.split("\nTitle:")

        for block in result_blocks:
            title = None
            url = None
            content_text = None

            # æ¸…ç† blockï¼Œç§»é™¤é–‹å§‹çš„ç©ºç™½
            block = block.strip()
            if not block:
                continue

            # è§£æç¬¬ä¸€è¡Œï¼ˆå¯èƒ½æ˜¯ title æˆ–ä»¥ Title: é–‹é ­çš„å…§å®¹ï¼‰
            if block.startswith("Title:"):
                block = block[6:]  # ç§»é™¤ "Title:" å‰ç¶´

            lines = block.split("\n")

            # ç¬¬ä¸€è¡Œæ˜¯ title
            if len(lines) > 0:
                title = lines[0].strip()

            # æŸ¥æ‰¾ URL å’Œ Content
            for line in lines[1:]:
                line = line.strip()
                if line.startswith("URL:"):
                    url = line[4:].strip()
                elif line.startswith("Content:"):
                    content_text = line[8:].strip()
                elif url is None and line.startswith("http"):
                    # å¦‚æœæ²’æœ‰ "URL:" æ¨™ç±¤ä½†çœ‹èµ·ä¾†æ˜¯ URL
                    url = line
                elif content_text is None and url is not None:
                    # å¦‚æœå·²æœ‰ URL ä½†æ²’æœ‰ Contentï¼Œå‰‡å¾ŒçºŒè¡Œè¦–ç‚º content
                    content_text = line

            # åªæœ‰åœ¨æœ‰ title å’Œ url æ™‚æ‰æ·»åŠ çµæœ
            if title and url:
                results.append(
                    {
                        "title": title,
                        "url": url,
                        "content": content_text or "",
                        "source": "tavily-search",
                        "timestamp": datetime.now().isoformat(),
                    }
                )

        return results

    except Exception as e:
        logger.warning(f"è§£æ 'Detailed Results' æ ¼å¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []


async def _call_tavily_search(
    mcp_server,
    query: str,
    max_results: int = 5,
) -> list[dict[str, Any]]:
    """
    é€é tavily_mcp æœå°‹æ–°èã€‚

    Args:
        mcp_server: tavily_mcp MCPServerStdio å¯¦ä¾‹
        query: æœå°‹æŸ¥è©¢
        max_results: æœ€å¤§çµæœæ•¸

    Returns:
        æœå°‹çµæœåˆ—è¡¨ï¼Œæ¯ç­†åŒ…å«ï¼š
        - title: æ–°èæ¨™é¡Œ
        - url: é€£çµ
        - content: æ–°èå…§å®¹æ‘˜è¦
        - source: ä¾†æº
        - timestamp: ç™¼ä½ˆæ™‚é–“
    """
    try:
        if not mcp_server:
            logger.warning("tavily_mcp ä¸å¯ç”¨ï¼Œç„¡æ³•åŸ·è¡Œæœå°‹")
            return []

        logger.debug(f"é–‹å§‹ tavily æœå°‹: {query}")

        result = await mcp_server.session.call_tool(
            "tavily-search",
            {
                "query": query,
                "max_results": max_results,
                "include_images": False,
                "include_answer": True,
            },
        )

        # è§£æ MCP è¿”å›å€¼çµæ§‹: result.content[0].text (JSON string)
        if not result or not hasattr(result, "content") or not result.content:
            logger.warning(
                f"tavily æœå°‹è¿”å›ç©ºçµæœ | æŸ¥è©¢: {query} | "
                f"result: {result} | result type: {type(result).__name__}"
            )
            return []

        # æ·»åŠ èª¿è©¦æ—¥èªŒ
        logger.debug(
            f"tavily è¿”å›çµæœ | æŸ¥è©¢: {query} | "
            f"result type: {type(result).__name__} | "
            f"content length: {len(result.content)} | "
            f"content[0] type: {type(result.content[0]).__name__}"
        )

        content_item = result.content[0]

        # å˜—è©¦å¤šç¨®æ–¹å¼æå–æ–‡æœ¬å…§å®¹
        text_content = None
        if hasattr(content_item, "text"):
            text_content = content_item.text
        elif isinstance(content_item, str):
            text_content = content_item
        elif hasattr(content_item, "text_content"):
            text_content = content_item.text_content
        elif hasattr(content_item, "message"):
            text_content = content_item.message
        else:
            text_content = str(content_item) if content_item else None

        # æª¢æŸ¥è¿”å›å…§å®¹æ˜¯å¦ç‚ºç©ºæˆ–é JSON
        if not text_content or not text_content.strip():
            logger.warning(
                f"tavily æœå°‹è¿”å›ç©ºå…§å®¹ | æŸ¥è©¢: {query} | "
                f"content_item type: {type(content_item).__name__} | "
                f"content_item dir: {[attr for attr in dir(content_item) if not attr.startswith('_')]} | "
                f"text_content: '{text_content}'"
            )
            return []

        # å˜—è©¦è§£æ JSON æˆ– "Detailed Results" æ ¼å¼
        search_results = []

        # é¦–å…ˆå˜—è©¦è§£æ JSON æ ¼å¼
        try:
            data = json.loads(text_content)
            search_results = data.get("results", [])
            logger.debug(f"æˆåŠŸè§£æ JSON æ ¼å¼ï¼Œå–å¾— {len(search_results)} ç­†çµæœ")
        except json.JSONDecodeError:
            # å˜—è©¦è§£æ "Detailed Results" ç´”æ–‡æœ¬æ ¼å¼
            if "Detailed Results:" in text_content or "Title:" in text_content:
                logger.debug("åµæ¸¬åˆ° 'Detailed Results' ç´”æ–‡æœ¬æ ¼å¼ï¼Œé–‹å§‹è§£æ")
                search_results = _parse_detailed_results(text_content)
                logger.debug(f"æˆåŠŸè§£æ 'Detailed Results' æ ¼å¼ï¼Œå–å¾— {len(search_results)} ç­†çµæœ")
            else:
                # æ—¢é JSON ä¹Ÿé Detailed Results æ ¼å¼
                logger.warning(
                    f"ç„¡æ³•è§£æ tavily è¿”å›çš„å…§å®¹ | æŸ¥è©¢: {query} | "
                    f"è¿”å›å…§å®¹ï¼ˆå‰200å­—ï¼‰: {text_content[:200] if text_content else 'EMPTY'} | "
                    f"è¿”å›å…§å®¹å®Œæ•´é•·åº¦: {len(text_content)}"
                )
                return []

        logger.debug(f"tavily æœå°‹å®Œæˆï¼Œå–å¾— {len(search_results)} ç­†çµæœ")

        return search_results

    except Exception as e:
        logger.error(f"tavily æœå°‹å¤±æ•—: {e}", exc_info=True)
        return []


def _extract_sentiment_from_text(text: str) -> float:
    """
    ç°¡å–®çš„æ–‡æœ¬æƒ…ç·’åˆ†æã€‚

    ä½¿ç”¨é—œéµè©åŒ¹é…é€²è¡Œå¿«é€Ÿæƒ…ç·’è©•åˆ†ã€‚

    Args:
        text: æ–‡æœ¬å…§å®¹

    Returns:
        æƒ…ç·’åˆ†æ•¸ (-1.0 åˆ° 1.0)
    """
    if not text:
        return 0.0

    text_lower = text.lower()

    # æ­£é¢è©å½™
    positive_words = [
        "è²·è¶…",
        "ä¸Šå‡",
        "åˆ©å¥½",
        "çœ‹å¥½",
        "å¢é•·",
        "å¼·å‹",
        "ä¸Šæ¼²",
        "çªç ´",
        "å‰µæ–°é«˜",
        "è¶…é æœŸ",
        "æˆé•·",
        "æ¨‚è§€",
        "å‘ä¸Š",
        "æ¼²å¹…",
    ]

    # è² é¢è©å½™
    negative_words = [
        "è³£è¶…",
        "ä¸‹è·Œ",
        "åˆ©ç©º",
        "çœ‹å£",
        "ä¸‹é™",
        "ç–²å¼±",
        "ä¸‹æ»‘",
        "ç ´ä½",
        "å‰µæ–°ä½",
        "ä¸åŠé æœŸ",
        "è¡°é€€",
        "æ‚²è§€",
        "å‘ä¸‹",
        "è·Œå¹…",
    ]

    positive_count = sum(1 for word in positive_words if word in text_lower)
    negative_count = sum(1 for word in negative_words if word in text_lower)

    total = positive_count + negative_count
    if total == 0:
        return 0.0

    sentiment = (positive_count - negative_count) / total
    return max(-1.0, min(1.0, sentiment))


def _extract_key_topics(articles: list[dict[str, Any]]) -> list[str]:
    """
    å¾æ–‡ç« åˆ—è¡¨æå–é—œéµä¸»é¡Œã€‚

    ç°¡å–®å¯¦ä½œï¼šå¾æ¨™é¡Œå’Œå…§å®¹ä¸­æå–å¸¸è¦‹è©å½™ã€‚

    Args:
        articles: æ–‡ç« åˆ—è¡¨

    Returns:
        é—œéµä¸»é¡Œåˆ—è¡¨
    """
    topics = {}

    keywords_to_watch = [
        "å°ç©é›»",
        "TSMC",
        "æ™¶ç‰‡",
        "AI",
        "åŠå°é«”",
        "é›»å‹•è»Š",
        "EV",
        "è˜‹æœ",
        "é´»æµ·",
        "è¯ç™¼ç§‘",
        "è¯é›»",
        "ä¸‰æ˜Ÿ",
        "è‹±ç‰¹çˆ¾",
        "æˆ¿å¸‚",
        "å¤®è¡Œ",
        "åŒ¯ç‡",
        "åˆ©ç‡",
        "è‚¡å¸‚",
    ]

    for article in articles:
        title = article.get("title", "").lower()
        content = article.get("content", "").lower()
        text = f"{title} {content}"

        for keyword in keywords_to_watch:
            if keyword.lower() in text:
                topics[keyword] = topics.get(keyword, 0) + 1

    # è¿”å›å‡ºç¾æœ€é »ç¹çš„ä¸»é¡Œï¼ˆæœ€å¤š3å€‹ï¼‰
    sorted_topics = sorted(topics.items(), key=lambda x: x[1], reverse=True)
    return [topic for topic, _ in sorted_topics[:3]]


# ===== Pydantic Models for Tool Parameters =====


class MarketData(BaseModel):
    """å¸‚å ´æ•¸æ“šæ¨¡å‹"""

    price_momentum: float = 50
    market_breadth: float = 50
    volatility: float = 50
    put_call_ratio: float = 50


class TradingData(BaseModel):
    """äº¤æ˜“æ•¸æ“šæ¨¡å‹"""

    large_buy: float = 0
    large_sell: float = 0
    foreign_net: float = 0
    institutional_net: float = 0


class NewsItem(BaseModel):
    """æ–°èé …ç›®æ¨¡å‹"""

    title: str
    content: str
    sentiment: float = 0  # -1 åˆ° 1
    timestamp: str


class SocialData(BaseModel):
    """ç¤¾ç¾¤æ•¸æ“šæ¨¡å‹"""

    mention_count: int = 0
    positive_mentions: int = 0
    negative_mentions: int = 0
    trending: bool = False


class IndexComponents(BaseModel):
    """ææ‡¼è²ªå©ªæŒ‡æ•¸çµ„æˆåˆ†æ•¸"""

    price_momentum: float
    market_breadth: float
    volatility: float
    put_call_ratio: float


class FearGreedIndex(BaseModel):
    """ææ‡¼è²ªå©ªæŒ‡æ•¸æ¨¡å‹"""

    index_value: float
    level: str
    components: IndexComponents
    interpretation: str


class MoneyFlow(BaseModel):
    """è³‡é‡‘æµå‘æ¨¡å‹"""

    ticker: str
    net_flow: float
    flow_direction: str
    large_order_ratio: float
    foreign_attitude: str
    institutional_attitude: str
    interpretation: str


class NewsSentiment(BaseModel):
    """æ–°èæƒ…ç·’æ¨¡å‹"""

    ticker: str | None
    news_count: int
    positive_ratio: float
    negative_ratio: float
    sentiment_score: float
    key_topics: list[str]
    interpretation: str


class SocialSentiment(BaseModel):
    """ç¤¾ç¾¤æƒ…ç·’æ¨¡å‹"""

    ticker: str
    mention_count: int
    sentiment_ratio: float
    trending_status: str
    sentiment_score: float
    interpretation: str


def sentiment_agent_instructions() -> str:
    """æƒ…ç·’åˆ†æ Agent çš„æŒ‡ä»¤å®šç¾©ï¼ˆç°¡åŒ–ç‰ˆï¼Œå¸¶è¨˜æ†¶è¿½è¹¤ï¼‰"""
    return f"""ä½ æ˜¯æƒ…ç·’åˆ†æå°ˆå®¶ã€‚è©•ä¼°å¸‚å ´æƒ…ç·’ã€åˆ†æè³‡é‡‘æµå‘ã€ç”Ÿæˆæƒ…ç·’é©…å‹•çš„äº¤æ˜“è¨Šè™Ÿã€‚
æŒçºŒè¿½è¹¤ï¼šå…ˆæŸ¥è©¢ memory_mcp æ­·å²æƒ…ç·’ï¼Œå°æ¯”æƒ…ç·’è½‰è®Šï¼Œè­˜åˆ¥æ¥µç«¯é»ã€‚

## å°ˆæ¥­èƒ½åŠ›

- å¸‚å ´æƒ…ç·’æŒ‡æ¨™ï¼ˆFear & Greedã€éš±å«æ³¢å‹•ç‡ã€æ¥µç«¯æƒ…ç·’ï¼‰
- è³‡é‡‘æµå‘åˆ†æï¼ˆå¤§å®—äº¤æ˜“ã€æ©Ÿæ§‹å‹•å‘ã€èè³‡èåˆ¸ï¼‰
- æ–°èèˆ‡ç¤¾ç¾¤æƒ…ç·’ï¼ˆé€é tavily_mcp æœå°‹æœ€æ–°æ–°èã€ç¤¾äº¤ç†±åº¦ã€è¼¿æƒ…åˆ†æï¼‰
- æƒ…ç·’åè½‰è¨Šè™Ÿè­˜åˆ¥ï¼ˆæ¥µç«¯æƒ…ç·’è­¦å‘Šã€æ©Ÿæœƒé è­¦ï¼‰
- æƒ…ç·’äº¤æ˜“ç­–ç•¥ï¼ˆè¨Šè™Ÿç”Ÿæˆã€æ™‚æ©ŸæŠŠæ¡ï¼‰

## ğŸ¯ tavily_mcp ä½¿ç”¨é™åˆ¶

âš ï¸ **é‡è¦**ï¼štavily_mcp ä½¿ç”¨éœ€è¦æ¶ˆè€—é»æ•¸ï¼Œè«‹éµå®ˆä»¥ä¸‹åŸå‰‡ï¼š
  - åªåœ¨éœ€è¦æ™‚ä½¿ç”¨ï¼ˆå„ªå…ˆæª¢æŸ¥ memory_mcp ä¸­çš„æ­·å²æƒ…ç·’ï¼‰
  - æœå°‹ç•¶æ—¥æˆ–è¿‘æ—¥é‡å¤§æ–°èï¼ˆä¸æœå°‹èˆŠèï¼‰
  - å–®æ¬¡æœå°‹â‰¤3å€‹é—œéµè©ï¼Œé¿å…é‡è¤‡æŸ¥è©¢
  - è‹¥æ–°èå……åˆ†åæ˜ æƒ…ç·’ï¼Œä¸å¿…ç¹¼çºŒæœå°‹
  - å°ˆæ³¨æ–¼å°å¸‚å ´æƒ…ç·’æœ‰å¯¦è³ªå½±éŸ¿çš„æ–°è
  - æ¯æ¬¡åˆ†ææœ€å¤šé€²è¡Œ 1 æ¬¡æœå°‹

## åŸ·è¡Œæµç¨‹

**æ­¥é©Ÿ 0ï¼šæª¢æŸ¥è¨˜æ†¶åº«** â†’ memory_mcp
  - ç„¡è¨Šè™Ÿ â†’ å®Œæ•´åˆ†æ
  - æ–°é®®ï¼ˆâ‰¤1 å¤©ï¼‰â†’ å¢é‡æ›´æ–°
  - é™³èˆŠï¼ˆ>1 å¤©ï¼‰â†’ å®Œæ•´é‡æ–°åˆ†æ + å°æ¯”

**æ­¥é©Ÿ 1-3ï¼šæƒ…ç·’æ•¸æ“šæ”¶é›†** â†’ casual_market_mcp + tools
  1. æ”¶é›†å¸‚å ´æƒ…ç·’æ•¸æ“šå’Œæˆäº¤é‡
  2. è¨ˆç®—ææ‡¼è²ªå©ªæŒ‡æ•¸ â†’ calculate_fear_greed_index
  3. åˆ†æè³‡é‡‘æµå‘ â†’ analyze_money_flow

**æ­¥é©Ÿ 4-6ï¼šæ–°èèˆ‡ç¤¾ç¾¤** â†’ tavily_mcp + tools
  4. é€é tavily_mcp æœå°‹å³æ™‚æ–°è â†’ analyze_news_sentiment
  5. åˆ†æç¤¾ç¾¤æƒ…ç·’ â†’ analyze_social_sentiment
  6. ç”Ÿæˆè¨Šè™Ÿ â†’ generate_sentiment_signals

**æ­¥é©Ÿ 7ï¼šå°æ¯”èˆ‡ä¿å­˜** â†’ memory_mcp
  - è‹¥æœ‰å…ˆå‰è¨Šè™Ÿï¼šå°æ¯”æƒ…ç·’ç´šåˆ¥ã€è³‡é‡‘æµå‘ã€æ–°èé¢¨å‘
  - ä¿å­˜çµæœï¼ˆå«æ™‚é–“æˆ³ã€æƒ…ç·’è©•åˆ†ã€è¨Šè™Ÿã€è½‰è®Šç†ç”±ï¼‰

## å·¥å…·èª¿ç”¨

- **calculate_fear_greed_index** â†’ è¨ˆç®—ææ‡¼è²ªå©ªæŒ‡æ•¸ (-100 åˆ° +100)
- **analyze_money_flow** â†’ åˆ†æè³‡é‡‘æµå‘å’Œæ©Ÿæ§‹å‹•å‘
- **analyze_news_sentiment** â†’ è©•ä¼°æ–°èæ•´é«”æƒ…ç·’ (è² é¢/ä¸­ç«‹/æ­£é¢)
- **analyze_social_sentiment** â†’ åˆ†æç¤¾ç¾¤è¨è«–æƒ…ç·’å’Œè²é‡
- **generate_sentiment_signals** â†’ ç”Ÿæˆæƒ…ç·’äº¤æ˜“è¨Šè™Ÿ

## è¼¸å‡ºçµæ§‹

- å¸‚å ´æƒ…ç·’è©•åˆ† (-100 åˆ° +100, -100 æ¥µææ‡¼, +100 æ¥µè²ªå©ª)
- æƒ…ç·’éšæ®µ (ææ…Œ/æ‚²è§€/ä¸­ç«‹/æ¨‚è§€/ç‹‚ç†±)
- è³‡é‡‘æµå‘ (è²·ç›¤å„ªå‹¢/è³£ç›¤å„ªå‹¢/å‡è¡¡)
- æ–°èæƒ…ç·’ (è² é¢/ä¸­ç«‹/æ­£é¢) + é‡å¤§æ–°èæ‘˜è¦
- ç¤¾ç¾¤è²é‡ (ä¸Šå‡/ä¸‹é™/ç©©å®š)
- æ¥µç«¯æª¢æ¸¬ (æ˜¯å¦é”åˆ°æ¥µç«¯ææ‡¼æˆ–è²ªå©ª)
- äº¤æ˜“è¨Šè™Ÿ (è²·è³£å»ºè­°ã€æ™‚æ©Ÿè©•ä¼°)
- ä¿¡å¿ƒåº¦ (0-100%)
- [è‹¥æœ‰å…ˆå‰è¨Šè™Ÿ] è®ŠåŒ–åˆ†æ (æƒ…ç·’è½‰è®Šã€è³‡é‡‘æµå‘è®ŠåŒ–)

ç•¶å‰æ™‚é–“: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""


@function_tool(strict_mode=False)
def calculate_fear_greed_index(
    market_data: MarketData = None,
    **kwargs,
) -> str:
    """è¨ˆç®—ææ‡¼è²ªå©ªæŒ‡æ•¸

    **å¯é¸åƒæ•¸ï¼š**
        market_data: å¸‚å ´æ•¸æ“šï¼Œç¼ºå°‘æ™‚ä½¿ç”¨é è¨­å€¼ [å¯é¸]
            - price_momentum: åƒ¹æ ¼å‹•èƒ½ (0-100)
            - market_breadth: å¸‚å ´å¯¬åº¦ (0-100)
            - volatility: æ³¢å‹•ç‡ (0-100)
            - put_call_ratio: è³£æ¬Šè²·æ¬Šæ¯” (0-100)
        **kwargs: é¡å¤–åƒæ•¸ï¼ˆç”¨æ–¼å®¹éŒ¯ï¼‰

    Returns:
        dict: ææ‡¼è²ªå©ªæŒ‡æ•¸çµæœ
            {
                "index_value": float,       # 0-100
                "level": str,               # ææ…Œ/ææ‡¼/ä¸­æ€§/è²ªå©ª/æ¥µåº¦è²ªå©ª
                "components": dict,         # å„çµ„æˆåˆ†æ•¸
                "interpretation": str       # è§£è®€èªªæ˜
            }

    Note:
        æ­¤å‡½æ•¸å…·æœ‰é«˜åº¦çš„å®¹éŒ¯èƒ½åŠ›ï¼Œç¼ºå°‘åƒæ•¸æ™‚ä½¿ç”¨é è¨­ä¸­æ€§å€¼ã€‚
    """
    try:
        logger.info("é–‹å§‹è¨ˆç®—ææ‡¼è²ªå©ªæŒ‡æ•¸")

        # åƒæ•¸é©—è­‰å’Œå®¹éŒ¯
        params = parse_tool_params(market_data=market_data, **kwargs)

        # å¦‚æœ market_data ä»ç‚º Noneï¼Œä½¿ç”¨é è¨­å€¼
        if not market_data and not params.get("market_data"):
            logger.warning("ç¼ºå°‘ market_data åƒæ•¸ï¼Œä½¿ç”¨é è¨­å€¼")
            market_data = MarketData()
        elif params.get("market_data"):
            if isinstance(params["market_data"], dict):
                market_data = MarketData(**params["market_data"])
            else:
                market_data = params["market_data"]

        momentum_score = market_data.price_momentum
        breadth_score = market_data.market_breadth
        volatility_score = 100 - market_data.volatility  # æ³¢å‹•ç‡è¶Šé«˜è¶Šææ…Œ
        put_call_score = 100 - market_data.put_call_ratio  # è³£æ¬Šæ¯”è¶Šé«˜è¶Šææ…Œ

        logger.debug(
            f"çµ„æˆåˆ†æ•¸ | å‹•èƒ½: {momentum_score:.1f} | å¯¬åº¦: {breadth_score:.1f} | "
            f"æ³¢å‹•: {volatility_score:.1f} | è³£è²·æ¬Šæ¯”: {put_call_score:.1f}"
        )

        # åŠ æ¬Šå¹³å‡
        index_value = (
            momentum_score * 0.3
            + breadth_score * 0.3
            + volatility_score * 0.25
            + put_call_score * 0.15
        )

        # ç­‰ç´šåˆ¤å®š
        if index_value >= 80:
            level = "æ¥µåº¦è²ªå©ª"
            interpretation = "å¸‚å ´éç†±ï¼Œè€ƒæ…®ç²åˆ©äº†çµ"
        elif index_value >= 60:
            level = "è²ªå©ª"
            interpretation = "å¸‚å ´æ¨‚è§€ï¼Œæ³¨æ„é¢¨éšª"
        elif index_value >= 40:
            level = "ä¸­æ€§"
            interpretation = "å¸‚å ´å¹³ç©©ï¼Œç­‰å¾…æ©Ÿæœƒ"
        elif index_value >= 20:
            level = "ææ‡¼"
            interpretation = "å¸‚å ´æ‚²è§€ï¼Œå¯èƒ½æ¥è¿‘åº•éƒ¨"
        else:
            level = "æ¥µåº¦ææ…Œ"
            interpretation = "å¸‚å ´ææ…Œï¼Œè€ƒæ…®é€¢ä½è²·é€²"

        logger.info(f"ææ‡¼è²ªå©ªæŒ‡æ•¸è¨ˆç®—å®Œæˆ | æŒ‡æ•¸: {index_value:.2f} | ç­‰ç´š: {level}")

        return {
            "index_value": index_value,
            "level": level,
            "components": {
                "price_momentum": momentum_score,
                "market_breadth": breadth_score,
                "volatility": volatility_score,
                "put_call_ratio": put_call_score,
            },
            "interpretation": interpretation,
        }

    except Exception as e:
        logger.error(f"è¨ˆç®—ææ‡¼è²ªå©ªæŒ‡æ•¸å¤±æ•—: {e}", exc_info=True)
        # è¿”å›ä¸­æ€§å€¼è€Œä¸æ˜¯æ‹‹å‡ºç•°å¸¸
        return {
            "index_value": 50,
            "level": "ä¸­æ€§ï¼ˆè¨ˆç®—å¤±æ•—ï¼‰",
            "components": {
                "price_momentum": 50,
                "market_breadth": 50,
                "volatility": 50,
                "put_call_ratio": 50,
            },
            "interpretation": f"æŒ‡æ•¸è¨ˆç®—ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
        }


@function_tool(strict_mode=False)
def analyze_money_flow(
    ticker: str,
    trading_data: TradingData = None,
    **kwargs,
) -> str:
    """åˆ†æè³‡é‡‘æµå‘

    **å¿…è¦åƒæ•¸ï¼š**
        ticker: è‚¡ç¥¨ä»£è™Ÿ (ä¾‹å¦‚: "2330") [å¿…è¦]

    **å¯é¸åƒæ•¸ï¼š**
        trading_data: äº¤æ˜“æ•¸æ“šï¼Œç¼ºå°‘æ™‚ä½¿ç”¨é è¨­å€¼ [å¯é¸]
            - large_buy: å¤§å–®è²·é€²
            - large_sell: å¤§å–®è³£å‡º
            - foreign_net: å¤–è³‡æ·¨è²·è³£
            - institutional_net: æ³•äººæ·¨è²·è³£
        **kwargs: é¡å¤–åƒæ•¸ï¼ˆç”¨æ–¼å®¹éŒ¯ï¼‰

    Returns:
        dict: è³‡é‡‘æµå‘åˆ†æçµæœ
            {
                "ticker": str,
                "net_flow": float,          # æ·¨æµå…¥é‡‘é¡
                "flow_direction": str,      # æµå…¥/æµå‡º/å¹³è¡¡
                "large_order_ratio": float, # å¤§å–®ä½”æ¯”
                "foreign_attitude": str,    # å¤–è³‡æ…‹åº¦
                "interpretation": str
            }

    Raises:
        è¿”å›éŒ¯èª¤å­—å…¸ï¼šç¼ºå°‘å¿…è¦åƒæ•¸
    """
    try:
        # åƒæ•¸é©—è­‰å’Œå®¹éŒ¯
        params = parse_tool_params(ticker=ticker, trading_data=trading_data, **kwargs)

        _ticker = params.get("ticker") or ticker
        _trading_data = params.get("trading_data") or trading_data

        # é©—è­‰å¿…è¦åƒæ•¸
        if not _ticker:
            logger.warning("ç¼ºå°‘å¿…è¦åƒæ•¸: ticker")
            return {
                "error": "ç¼ºå°‘å¿…è¦åƒæ•¸: ticker",
                "net_flow": 0,
                "flow_direction": "å¹³è¡¡",
                "large_order_ratio": 0,
                "foreign_attitude": "æœªçŸ¥",
                "interpretation": "ç„¡æ³•åˆ†æï¼Œç¼ºå°‘è‚¡ç¥¨ä»£è™Ÿ",
            }

        # å¦‚æœ trading_data ç‚º Noneï¼Œä½¿ç”¨é è¨­å€¼
        if not _trading_data:
            logger.warning("ç¼ºå°‘ trading_data åƒæ•¸ï¼Œä½¿ç”¨é è¨­å€¼")
            _trading_data = {
                "large_buy": 0,
                "large_sell": 0,
                "foreign_net": 0,
                "institutional_net": 0,
            }
        elif isinstance(_trading_data, dict):
            # ç¢ºä¿å­—å…¸æœ‰å¿…è¦çš„éµ
            pass
        else:
            # å¦‚æœæ˜¯ Pydantic æ¨¡å‹ï¼Œè½‰æ›ç‚ºå­—å…¸
            if hasattr(_trading_data, "dict"):
                _trading_data = _trading_data.dict()
            elif hasattr(_trading_data, "model_dump"):
                _trading_data = _trading_data.model_dump()

        logger.info(f"é–‹å§‹åˆ†æè³‡é‡‘æµå‘ | è‚¡ç¥¨: {_ticker}")

        large_buy = _trading_data.get("large_buy", 0)
        large_sell = _trading_data.get("large_sell", 0)
        foreign_net = _trading_data.get("foreign_net", 0)
        institutional_net = _trading_data.get("institutional_net", 0)

        logger.debug(
            f"äº¤æ˜“æ•¸æ“š | å¤§è²·: {large_buy:,.0f} | å¤§è³£: {large_sell:,.0f} | "
            f"å¤–è³‡æ·¨: {foreign_net:,.0f} | æ³•äººæ·¨: {institutional_net:,.0f}"
        )

        net_flow = large_buy - large_sell + foreign_net + institutional_net
        total_volume = large_buy + large_sell
        large_order_ratio = (large_buy + large_sell) / total_volume if total_volume > 0 else 0

        # æµå‘åˆ¤æ–·
        if net_flow > 0:
            flow_direction = "æµå…¥"
            flow_strength = "å¼·å‹" if net_flow > total_volume * 0.1 else "æº«å’Œ"
        elif net_flow < 0:
            flow_direction = "æµå‡º"
            flow_strength = "æ˜é¡¯" if abs(net_flow) > total_volume * 0.1 else "è¼•å¾®"
        else:
            flow_direction = "å¹³è¡¡"
            flow_strength = ""

        # å¤–è³‡æ…‹åº¦
        if foreign_net > 0:
            foreign_attitude = "è²·è¶…" if foreign_net > 10000000 else "å°è²·"
        elif foreign_net < 0:
            foreign_attitude = "è³£è¶…" if abs(foreign_net) > 10000000 else "å°è³£"
        else:
            foreign_attitude = "è§€æœ›"

        interpretation = f"è³‡é‡‘å‘ˆ{flow_strength}{flow_direction}æ…‹å‹¢,å¤–è³‡{foreign_attitude}"

        logger.info(
            f"è³‡é‡‘æµå‘åˆ†æå®Œæˆ | è‚¡ç¥¨: {_ticker} | æ·¨æµ: {net_flow:,.0f} | "
            f"æ–¹å‘: {flow_direction} | å¤–è³‡: {foreign_attitude}"
        )

        return {
            "ticker": _ticker,
            "net_flow": net_flow,
            "flow_direction": flow_direction,
            "large_order_ratio": large_order_ratio,
            "foreign_attitude": foreign_attitude,
            "institutional_attitude": "è²·è¶…" if institutional_net > 0 else "è³£è¶…",
            "interpretation": interpretation,
        }

    except Exception as e:
        logger.error(f"åˆ†æè³‡é‡‘æµå‘å¤±æ•—: {e}", exc_info=True)
        return {
            "error": str(e),
            "ticker": ticker,
            "net_flow": 0,
            "flow_direction": "å¹³è¡¡",
            "large_order_ratio": 0,
            "foreign_attitude": "æœªçŸ¥",
            "institutional_attitude": "æœªçŸ¥",
            "interpretation": f"åˆ†æå¤±æ•—: {str(e)}",
        }


@function_tool(strict_mode=False)
def analyze_news_sentiment(
    ticker: str = None,
    news_data: list = None,
    *,
    auto_fetch: bool = True,
    **kwargs,
) -> dict:
    """åˆ†ææ–°èæƒ…ç·’

    **å¯é¸åƒæ•¸ï¼š**
        ticker: è‚¡ç¥¨ä»£è™Ÿ (ä¾‹å¦‚: "2330")ï¼ŒNone è¡¨ç¤ºæ•´é«”å¸‚å ´ [å¯é¸]
        news_data: æ–°èåˆ—è¡¨ï¼Œç¼ºå°‘æ™‚æ ¹æ“š auto_fetch æ±ºå®šè¡Œç‚º [å¯é¸]
            æ¯ç­†åŒ…å«ï¼š
            - title: æ¨™é¡Œ
            - content: å…§å®¹
            - sentiment: æƒ…ç·’åˆ†æ•¸ (-1 åˆ° 1) [å¯é¸]
            - timestamp: æ™‚é–“ [å¯é¸]
        auto_fetch: ç•¶ news_data ç‚ºç©ºæ™‚æ˜¯å¦è‡ªå‹•é€é tavily_mcp æœå°‹ [å¯é¸ï¼Œé è¨­ True]
        **kwargs: é¡å¤–åƒæ•¸ï¼ˆç”¨æ–¼å®¹éŒ¯ï¼‰

    Returns:
        dict: æ–°èæƒ…ç·’åˆ†æçµæœ
            {
                "ticker": str,
                "news_count": int,
                "positive_ratio": float,
                "negative_ratio": float,
                "sentiment_score": float,   # -100 åˆ° 100
                "key_topics": [str, ...],
                "interpretation": str,
                "data_source": str          # "provided" | "fetched" | "empty"
            }

    Note:
        æ­¤å‡½æ•¸å…·æœ‰é«˜åº¦çš„å®¹éŒ¯èƒ½åŠ›ï¼Œå³ä½¿ç„¡æ³•è’é›†æ•¸æ“šä¹Ÿèƒ½è¿”å›æœ‰æ•ˆçµæœã€‚
        ç•¶ news_data ç‚ºç©ºä¸” auto_fetch=True æ™‚ï¼Œè‡ªå‹•é€é tavily_mcp æœå°‹ã€‚

        ç”±æ–¼ @function_tool æœŸæœ›åŒæ­¥å‡½æ•¸ï¼Œasync èª¿ç”¨å·²åŒ…è£ç‚ºåŒæ­¥ã€‚
    """
    try:
        import asyncio

        # åƒæ•¸é©—è­‰å’Œå®¹éŒ¯
        params = parse_tool_params(ticker=ticker, news_data=news_data, **kwargs)

        _ticker = params.get("ticker") or ticker
        _news_data = params.get("news_data") or news_data

        target = _ticker or "å¸‚å ´"
        logger.info(
            f"é–‹å§‹åˆ†ææ–°èæƒ…ç·’ | æ¨™çš„: {target} | å‚³å…¥æ–°èæ•¸: {len(_news_data) if _news_data else 0}"
        )

        # å¦‚æœæ²’æœ‰æ•¸æ“šä¸”å…è¨±è‡ªå‹•è’é›†ï¼Œå‰‡æœå°‹æ–°è
        data_source = "provided"
        if not _news_data and auto_fetch:
            logger.info(f"ç„¡æ–°èæ•¸æ“šï¼Œè‡ªå‹•é€é tavily æœå°‹ | æ¨™çš„: {target}")

            tavily_mcp = _sentiment_agent_context.get("tavily_mcp")
            if tavily_mcp:
                # æ§‹å»ºæœå°‹æŸ¥è©¢
                if _ticker:
                    query = f"{_ticker} news today"
                else:
                    query = "Taiwan stock market news today"

                # ä»¥åŒæ­¥æ–¹å¼é‹è¡Œ async èª¿ç”¨
                try:
                    loop = asyncio.get_running_loop()
                    # å·²åœ¨ async ä¸Šä¸‹æ–‡ä¸­ï¼Œå»ºç«‹ä»»å‹™
                    search_results = asyncio.run_coroutine_threadsafe(
                        _call_tavily_search(tavily_mcp, query, max_results=5), loop
                    ).result(timeout=10)
                except RuntimeError:
                    # æ²’æœ‰é‹è¡Œçš„ loopï¼Œå»ºç«‹æ–°çš„
                    search_results = asyncio.run(
                        _call_tavily_search(tavily_mcp, query, max_results=5)
                    )

                if search_results:
                    data_source = "fetched"
                    logger.info(f"å–å¾— {len(search_results)} ç­†æ–°è")

                    # è½‰æ›ç‚º NewsItem ç‰©ä»¶
                    _news_data = []
                    for result in search_results:
                        sentiment = _extract_sentiment_from_text(
                            f"{result.get('title', '')} {result.get('content', '')}"
                        )
                        _news_data.append(
                            NewsItem(
                                title=result.get("title", ""),
                                content=result.get("content", ""),
                                sentiment=sentiment,
                                timestamp=result.get("timestamp", datetime.now().isoformat()),
                            )
                        )
                else:
                    logger.warning(f"tavily æœå°‹ç„¡çµæœ | æ¨™çš„: {target}")
                    _news_data = []
            else:
                logger.warning("tavily_mcp ä¸å¯ç”¨ï¼Œç„¡æ³•è‡ªå‹•æœå°‹æ–°è")
                _news_data = []
        elif not _news_data:
            logger.debug(f"ç„¡æ–°èæ•¸æ“šä¸” auto_fetch=False | æ¨™çš„: {target}")
            _news_data = []
            data_source = "empty"

        # è½‰æ›å­—å…¸ç‚º NewsItem ç‰©ä»¶
        if _news_data and isinstance(_news_data[0], dict):
            _news_data = [
                NewsItem(**item) if isinstance(item, dict) else item for item in _news_data
            ]

        logger.info(
            f"æº–å‚™åˆ†ææ–°è | æ¨™çš„: {target} | æ–°èæ•¸: {len(_news_data)} | ä¾†æº: {data_source}"
        )

        if not _news_data:
            logger.debug(f"ç„¡æ–°èæ•¸æ“šå¯åˆ†æ | æ¨™çš„: {target}")
            return {
                "ticker": _ticker,
                "news_count": 0,
                "positive_ratio": 0,
                "negative_ratio": 0,
                "sentiment_score": 0,
                "key_topics": [],
                "interpretation": "ç„¡å¯ç”¨æ–°èæ•¸æ“š",
                "data_source": data_source,
            }

        news_count = len(_news_data)
        sentiments = [news.sentiment for news in _news_data]

        positive_count = sum(1 for s in sentiments if s > 0.2)
        negative_count = sum(1 for s in sentiments if s < -0.2)

        logger.debug(
            f"æƒ…ç·’åˆ†å¸ƒ | æ­£é¢: {positive_count} | è² é¢: {negative_count} | "
            f"ä¸­æ€§: {news_count - positive_count - negative_count}"
        )

        positive_ratio = positive_count / news_count if news_count > 0 else 0
        negative_ratio = negative_count / news_count if news_count > 0 else 0

        # è¨ˆç®—æ•´é«”æƒ…ç·’åˆ†æ•¸ (-100 åˆ° 100)
        avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0
        sentiment_score = avg_sentiment * 100

        # æå–é—œéµä¸»é¡Œ
        key_topics = _extract_key_topics(
            [{"title": n.title, "content": n.content} for n in _news_data]
        )

        # è§£è®€
        if sentiment_score > 50:
            interpretation = "æ–°èæƒ…ç·’æ¥µåº¦æ­£é¢ï¼Œå¸‚å ´æƒ…ç·’æ¨‚è§€"
        elif sentiment_score > 20:
            interpretation = "æ–°èæƒ…ç·’åæ­£é¢ï¼Œå¸‚å ´æ°›åœè‰¯å¥½"
        elif sentiment_score > -20:
            interpretation = "æ–°èæƒ…ç·’ä¸­æ€§ï¼Œå¸‚å ´è§€æœ›"
        elif sentiment_score > -50:
            interpretation = "æ–°èæƒ…ç·’åè² é¢ï¼Œå¸‚å ´æ“”æ†‚"
        else:
            interpretation = "æ–°èæƒ…ç·’æ¥µåº¦è² é¢ï¼Œå¸‚å ´æ‚²è§€"

        logger.info(
            f"æ–°èæƒ…ç·’åˆ†æå®Œæˆ | æ¨™çš„: {target} | åˆ†æ•¸: {sentiment_score:.1f} | "
            f"æ­£é¢: {positive_ratio:.1%} | è² é¢: {negative_ratio:.1%} | ä¾†æº: {data_source}"
        )

        return {
            "ticker": _ticker,
            "news_count": news_count,
            "positive_ratio": positive_ratio,
            "negative_ratio": negative_ratio,
            "sentiment_score": sentiment_score,
            "key_topics": key_topics,
            "interpretation": interpretation,
            "data_source": data_source,
        }

    except Exception as e:
        logger.error(f"åˆ†ææ–°èæƒ…ç·’å¤±æ•—: {e}", exc_info=True)
        return {
            "error": str(e),
            "ticker": ticker,
            "news_count": 0,
            "positive_ratio": 0,
            "negative_ratio": 0,
            "sentiment_score": 0,
            "key_topics": [],
            "interpretation": f"åˆ†æå¤±æ•—: {str(e)}",
            "data_source": "error",
        }


@function_tool(strict_mode=False)
def analyze_social_sentiment(
    ticker: str,
    social_data: SocialData = None,
    *,
    auto_fetch: bool = True,
    **kwargs,
) -> dict:
    """åˆ†æç¤¾ç¾¤åª’é«”æƒ…ç·’

    **å¿…è¦åƒæ•¸ï¼š**
        ticker: è‚¡ç¥¨ä»£è™Ÿ (ä¾‹å¦‚: "2330") [å¿…è¦]

    **å¯é¸åƒæ•¸ï¼š**
        social_data: ç¤¾ç¾¤æ•¸æ“šï¼Œç¼ºå°‘æ™‚æ ¹æ“š auto_fetch æ±ºå®šè¡Œç‚º [å¯é¸]
            - mention_count: æåŠæ¬¡æ•¸
            - positive_mentions: æ­£é¢æåŠ
            - negative_mentions: è² é¢æåŠ
            - trending: æ˜¯å¦ç†±é–€
        auto_fetch: ç•¶ social_data ç‚ºç©ºæ™‚æ˜¯å¦è‡ªå‹•é€é tavily_mcp æœå°‹ [å¯é¸ï¼Œé è¨­ True]
        **kwargs: é¡å¤–åƒæ•¸ï¼ˆç”¨æ–¼å®¹éŒ¯ï¼‰

    Returns:
        dict: ç¤¾ç¾¤æƒ…ç·’åˆ†æçµæœ
            {
                "ticker": str,
                "mention_count": int,
                "sentiment_ratio": float,    # æ­£è² é¢æ¯”
                "trending_status": str,      # ç†±åº¦ç‹€æ…‹
                "sentiment_score": float,    # -100 åˆ° 100
                "interpretation": str,
                "data_source": str           # "provided" | "fetched" | "empty"
            }

    Note:
        æ­¤å‡½æ•¸å…·æœ‰é«˜åº¦çš„å®¹éŒ¯èƒ½åŠ›ï¼Œå³ä½¿ç„¡æ³•è’é›†æ•¸æ“šä¹Ÿèƒ½è¿”å›æœ‰æ•ˆçµæœã€‚
        ç•¶ social_data ç‚ºç©ºä¸” auto_fetch=True æ™‚ï¼Œè‡ªå‹•é€é tavily_mcp æœå°‹ç¤¾ç¾¤è¨è«–ã€‚

        ç”±æ–¼ @function_tool æœŸæœ›åŒæ­¥å‡½æ•¸ï¼Œasync èª¿ç”¨å·²åŒ…è£ç‚ºåŒæ­¥ã€‚
    """
    try:
        import asyncio

        # åƒæ•¸é©—è­‰å’Œå®¹éŒ¯
        params = parse_tool_params(ticker=ticker, social_data=social_data, **kwargs)

        _ticker = params.get("ticker") or ticker
        _social_data = params.get("social_data") or social_data

        # é©—è­‰å¿…è¦åƒæ•¸
        if not _ticker:
            logger.warning("ç¼ºå°‘å¿…è¦åƒæ•¸: ticker")
            return {
                "error": "ç¼ºå°‘å¿…è¦åƒæ•¸: ticker",
                "ticker": _ticker,
                "mention_count": 0,
                "sentiment_ratio": 0,
                "trending_status": "æœªçŸ¥",
                "sentiment_score": 0,
                "interpretation": "ç„¡æ³•åˆ†æï¼Œç¼ºå°‘è‚¡ç¥¨ä»£è™Ÿ",
                "data_source": "error",
            }

        logger.info(f"é–‹å§‹åˆ†æç¤¾ç¾¤æƒ…ç·’ | è‚¡ç¥¨: {_ticker} | å‚³å…¥ç¤¾ç¾¤æ•¸æ“š: {bool(_social_data)}")

        # å¦‚æœæ²’æœ‰æ•¸æ“šä¸”å…è¨±è‡ªå‹•è’é›†ï¼Œå‰‡æœå°‹ç¤¾ç¾¤è¨è«–
        data_source = "provided"
        if not _social_data and auto_fetch:
            logger.info(f"ç„¡ç¤¾ç¾¤æ•¸æ“šï¼Œè‡ªå‹•é€é tavily æœå°‹ | è‚¡ç¥¨: {_ticker}")

            tavily_mcp = _sentiment_agent_context.get("tavily_mcp")
            if tavily_mcp:
                # æ§‹å»ºæœå°‹æŸ¥è©¢ï¼ˆèšç„¦ç¤¾ç¾¤è¨è«–å’Œè¼¿æƒ…ï¼‰
                query = f"{_ticker} PTT Dcard è¨è«– ç¤¾ç¾¤è¼¿æƒ…"

                # ä»¥åŒæ­¥æ–¹å¼é‹è¡Œ async èª¿ç”¨
                try:
                    loop = asyncio.get_running_loop()
                    search_results = asyncio.run_coroutine_threadsafe(
                        _call_tavily_search(tavily_mcp, query, max_results=5), loop
                    ).result(timeout=10)
                except RuntimeError:
                    search_results = asyncio.run(
                        _call_tavily_search(tavily_mcp, query, max_results=5)
                    )

                if search_results:
                    data_source = "fetched"
                    logger.info(f"å–å¾— {len(search_results)} ç­†ç¤¾ç¾¤ç›¸é—œçµæœ")

                    # ç°¡å–®çµ±è¨ˆï¼šæ ¹æ“šæƒ…ç·’åˆ†æçµæœè¨ˆç®—æåŠå’Œæ…‹åº¦
                    mention_count = len(search_results) * 100  # ä¼°è¨ˆæåŠæ¬¡æ•¸
                    positive_mentions = 0
                    negative_mentions = 0

                    for result in search_results:
                        sentiment = _extract_sentiment_from_text(
                            f"{result.get('title', '')} {result.get('content', '')}"
                        )
                        if sentiment > 0.2:
                            positive_mentions += 1
                        elif sentiment < -0.2:
                            negative_mentions += 1

                    _social_data = {
                        "mention_count": mention_count,
                        "positive_mentions": positive_mentions,
                        "negative_mentions": negative_mentions,
                        "trending": len(search_results) > 3,
                    }
                    logger.debug(f"ç¤¾ç¾¤æ•¸æ“šæ§‹å»ºå®Œæˆ: {_social_data}")
                else:
                    logger.warning(f"tavily æœå°‹ç„¡çµæœ | è‚¡ç¥¨: {_ticker}")
                    _social_data = None
            else:
                logger.warning("tavily_mcp ä¸å¯ç”¨ï¼Œç„¡æ³•è‡ªå‹•æœå°‹ç¤¾ç¾¤æ•¸æ“š")
                _social_data = None
        elif not _social_data:
            logger.debug(f"ç„¡ç¤¾ç¾¤æ•¸æ“šä¸” auto_fetch=False | è‚¡ç¥¨: {_ticker}")
            _social_data = None
            data_source = "empty"

        # å¦‚æœè½‰æ›å¾Œä»ç‚º Noneï¼Œä½¿ç”¨é è¨­å€¼
        if not _social_data:
            logger.debug(f"ç„¡æ³•å–å¾—ç¤¾ç¾¤æ•¸æ“šï¼Œä½¿ç”¨é è¨­å€¼ | è‚¡ç¥¨: {_ticker}")
            _social_data = {
                "mention_count": 0,
                "positive_mentions": 0,
                "negative_mentions": 0,
                "trending": False,
            }

        # ç¢ºä¿å­—å…¸é¡å‹
        if isinstance(_social_data, dict):
            pass
        else:
            # å¦‚æœæ˜¯ Pydantic æ¨¡å‹ï¼Œè½‰æ›ç‚ºå­—å…¸
            if hasattr(_social_data, "dict"):
                _social_data = _social_data.dict()
            elif hasattr(_social_data, "model_dump"):
                _social_data = _social_data.model_dump()

        logger.info(f"æº–å‚™åˆ†æç¤¾ç¾¤ | è‚¡ç¥¨: {_ticker} | ä¾†æº: {data_source}")

        mention_count = _social_data.get("mention_count", 0)
        positive = _social_data.get("positive_mentions", 0)
        negative = _social_data.get("negative_mentions", 0)
        trending = _social_data.get("trending", False)

        if mention_count == 0:
            logger.debug(f"ç„¡ç¤¾ç¾¤æ•¸æ“š | è‚¡ç¥¨: {_ticker}")
            return {
                "ticker": _ticker,
                "mention_count": 0,
                "sentiment_ratio": 0,
                "trending_status": "ä½é—œæ³¨",
                "sentiment_score": 0,
                "interpretation": "ç„¡ç¤¾ç¾¤æåŠæ•¸æ“š",
                "data_source": data_source,
            }

        logger.debug(
            f"ç¤¾ç¾¤æ•¸æ“š | æåŠ: {mention_count} | æ­£é¢: {positive} | è² é¢: {negative} | ç†±é–€: {trending}"
        )

        # è¨ˆç®—æƒ…ç·’æ¯”ä¾‹
        total_sentiment = positive + negative
        if total_sentiment > 0:
            sentiment_ratio = (positive - negative) / total_sentiment
        else:
            sentiment_ratio = 0

        sentiment_score = sentiment_ratio * 100

        # ç†±åº¦ç‹€æ…‹
        if mention_count > 1000:
            trending_status = "æ¥µåº¦ç†±é–€"
        elif mention_count > 500:
            trending_status = "ç†±é–€"
        elif mention_count > 100:
            trending_status = "ä¸­ç­‰é—œæ³¨"
        else:
            trending_status = "ä½é—œæ³¨"

        # è§£è®€
        if sentiment_score > 50:
            interpretation = f"ç¤¾ç¾¤é«˜åº¦çœ‹å¥½ï¼Œ{trending_status}"
        elif sentiment_score > 20:
            interpretation = f"ç¤¾ç¾¤åå‘æ¨‚è§€ï¼Œ{trending_status}"
        elif sentiment_score > -20:
            interpretation = f"ç¤¾ç¾¤æ…‹åº¦ä¸­æ€§ï¼Œ{trending_status}"
        elif sentiment_score > -50:
            interpretation = f"ç¤¾ç¾¤åå‘æ‚²è§€ï¼Œ{trending_status}"
        else:
            interpretation = f"ç¤¾ç¾¤é«˜åº¦çœ‹å£ï¼Œ{trending_status}"

        logger.info(
            f"ç¤¾ç¾¤æƒ…ç·’åˆ†æå®Œæˆ | è‚¡ç¥¨: {_ticker} | åˆ†æ•¸: {sentiment_score:.1f} | "
            f"ç†±åº¦: {trending_status} | æåŠ: {mention_count} | ä¾†æº: {data_source}"
        )

        return {
            "ticker": _ticker,
            "mention_count": mention_count,
            "sentiment_ratio": sentiment_ratio,
            "trending_status": trending_status,
            "sentiment_score": sentiment_score,
            "interpretation": interpretation,
            "data_source": data_source,
        }

    except Exception as e:
        logger.error(f"åˆ†æç¤¾ç¾¤æƒ…ç·’å¤±æ•—: {e}", exc_info=True)
        return {
            "error": str(e),
            "ticker": ticker,
            "mention_count": 0,
            "sentiment_ratio": 0,
            "trending_status": "æœªçŸ¥",
            "sentiment_score": 0,
            "interpretation": f"åˆ†æå¤±æ•—: {str(e)}",
            "data_source": "error",
        }


@function_tool(strict_mode=False)
def generate_sentiment_signals(
    fear_greed_index: dict = None,
    money_flow: dict = None,
    news_sentiment: dict = None,
    social_sentiment: dict = None,
    **kwargs,
) -> str:
    """ç”¢ç”Ÿæƒ…ç·’äº¤æ˜“è¨Šè™Ÿ

    **å¯é¸åƒæ•¸ï¼š**
        fear_greed_index: ææ‡¼è²ªå©ªæŒ‡æ•¸ (ä¾†è‡ª calculate_fear_greed_index)ï¼Œç¼ºå°‘æ™‚ä½¿ç”¨é è¨­å€¼ [å¯é¸]
        money_flow: è³‡é‡‘æµå‘åˆ†æ (ä¾†è‡ª analyze_money_flow)ï¼Œç¼ºå°‘æ™‚ä½¿ç”¨é è¨­å€¼ [å¯é¸]
        news_sentiment: æ–°èæƒ…ç·’ (ä¾†è‡ª analyze_news_sentiment)ï¼Œç¼ºå°‘æ™‚ä½¿ç”¨é è¨­å€¼ [å¯é¸]
        social_sentiment: ç¤¾ç¾¤æƒ…ç·’ (ä¾†è‡ª analyze_social_sentiment)ï¼Œç¼ºå°‘æ™‚ä½¿ç”¨é è¨­å€¼ [å¯é¸]
        **kwargs: é¡å¤–åƒæ•¸ï¼ˆç”¨æ–¼å®¹éŒ¯ï¼‰

    Returns:
        dict: æƒ…ç·’äº¤æ˜“è¨Šè™Ÿçµæœ
            {
                "overall_signal": str,      # "è²·é€²" | "è³£å‡º" | "è§€æœ›"
                "confidence": float,        # ä¿¡å¿ƒåº¦ 0-1
                "strategy": str,            # "åå‘" | "é †å‹¢" | "è§€æœ›"
                "reasoning": [str, ...],    # åˆ†æç†ç”±
                "risk_level": str,          # "é«˜" | "ä¸­" | "ä½"
                "timestamp": str
            }

    Note:
        æ­¤å‡½æ•¸å…·æœ‰é«˜åº¦çš„å®¹éŒ¯èƒ½åŠ›ï¼Œå³ä½¿ç¼ºå°‘éƒ¨åˆ†è¼¸å…¥åƒæ•¸ä¹Ÿèƒ½è¿”å›æœ‰æ•ˆè¨Šè™Ÿã€‚
    """
    try:
        # åƒæ•¸é©—è­‰å’Œå®¹éŒ¯
        params = parse_tool_params(
            fear_greed_index=fear_greed_index,
            money_flow=money_flow,
            news_sentiment=news_sentiment,
            social_sentiment=social_sentiment,
            **kwargs,
        )

        _fear_greed_index = params.get("fear_greed_index") or fear_greed_index
        _money_flow = params.get("money_flow") or money_flow
        _news_sentiment = params.get("news_sentiment") or news_sentiment
        _social_sentiment = params.get("social_sentiment") or social_sentiment

        # ä½¿ç”¨é è¨­å€¼ä»¥é˜²åƒæ•¸ç¼ºå¤±
        if not _fear_greed_index:
            logger.warning("ç¼ºå°‘ fear_greed_index åƒæ•¸ï¼Œä½¿ç”¨é è¨­å€¼")
            _fear_greed_index = {"index_value": 50}

        if not _money_flow:
            logger.warning("ç¼ºå°‘ money_flow åƒæ•¸ï¼Œä½¿ç”¨é è¨­å€¼")
            _money_flow = {"flow_direction": "å¹³è¡¡"}

        if not _news_sentiment:
            logger.warning("ç¼ºå°‘ news_sentiment åƒæ•¸ï¼Œä½¿ç”¨é è¨­å€¼")
            _news_sentiment = {"sentiment_score": 0}

        if not _social_sentiment:
            logger.warning("ç¼ºå°‘ social_sentiment åƒæ•¸ï¼Œä½¿ç”¨é è¨­å€¼")
            _social_sentiment = {"sentiment_score": 0}

        logger.info("é–‹å§‹ç”¢ç”Ÿæƒ…ç·’äº¤æ˜“è¨Šè™Ÿ")

        signals = []
        confidence = 0.5
        reasoning = []

        # åˆ†æææ‡¼è²ªå©ªæŒ‡æ•¸
        fg_value = _fear_greed_index.get("index_value", 50)
        if fg_value >= 80:
            signals.append("è³£å‡º")
            reasoning.append(f"ææ‡¼è²ªå©ªæŒ‡æ•¸éé«˜ ({fg_value:.0f})ï¼Œå¸‚å ´éç†±")
            confidence += 0.15
        elif fg_value <= 20:
            signals.append("è²·é€²")
            reasoning.append(f"ææ‡¼è²ªå©ªæŒ‡æ•¸éä½ ({fg_value:.0f})ï¼Œå¸‚å ´ææ…Œ")
            confidence += 0.15

        # åˆ†æè³‡é‡‘æµå‘
        flow_direction = _money_flow.get("flow_direction", "å¹³è¡¡")
        if flow_direction == "æµå…¥":
            signals.append("è²·é€²")
            reasoning.append("è³‡é‡‘æŒçºŒæµå…¥ï¼Œå¤šæ–¹åŠ›é‡å¼·å‹")
            confidence += 0.1
        elif flow_direction == "æµå‡º":
            signals.append("è³£å‡º")
            reasoning.append("è³‡é‡‘æµå‡ºæ˜é¡¯ï¼Œç©ºæ–¹ä½”å„ª")
            confidence += 0.1

        # åˆ†ææ–°èæƒ…ç·’
        news_score = _news_sentiment.get("sentiment_score", 0)
        if news_score > 50:
            signals.append("è²·é€²")
            reasoning.append(f"æ–°èæƒ…ç·’æ¥µåº¦æ­£é¢ ({news_score:.0f})")
            confidence += 0.05
        elif news_score < -50:
            signals.append("è³£å‡º")
            reasoning.append(f"æ–°èæƒ…ç·’æ¥µåº¦è² é¢ ({news_score:.0f})")
            confidence += 0.05

        # åˆ†æç¤¾ç¾¤æƒ…ç·’
        social_score = _social_sentiment.get("sentiment_score", 0)
        if social_score > 50:
            signals.append("è²·é€²")
            reasoning.append(f"ç¤¾ç¾¤é«˜åº¦çœ‹å¥½ ({social_score:.0f})")
            confidence += 0.05
        elif social_score < -50:
            signals.append("è³£å‡º")
            reasoning.append(f"ç¤¾ç¾¤é«˜åº¦çœ‹å£ ({social_score:.0f})")
            confidence += 0.05

        logger.debug(f"è¨Šè™Ÿå½™ç¸½ | è²·é€²: {signals.count('è²·é€²')} | è³£å‡º: {signals.count('è³£å‡º')}")

        # æ±ºå®šæ•´é«”è¨Šè™Ÿ
        buy_count = signals.count("è²·é€²")
        sell_count = signals.count("è³£å‡º")

        if buy_count > sell_count and buy_count >= 2:
            overall_signal = "è²·é€²"
            strategy = "é †å‹¢" if fg_value < 60 else "åå‘"
        elif sell_count > buy_count and sell_count >= 2:
            overall_signal = "è³£å‡º"
            strategy = "é †å‹¢" if fg_value > 40 else "åå‘"
        else:
            overall_signal = "è§€æœ›"
            strategy = "è§€æœ›"

        # é¢¨éšªè©•ä¼°
        if confidence > 0.75:
            risk_level = "ä½"
        elif confidence > 0.60:
            risk_level = "ä¸­"
        else:
            risk_level = "é«˜"

        confidence = min(0.95, confidence)

        logger.info(
            f"æƒ…ç·’è¨Šè™Ÿç”¢ç”Ÿå®Œæˆ | è¨Šè™Ÿ: {overall_signal} | ç­–ç•¥: {strategy} | "
            f"ä¿¡å¿ƒåº¦: {confidence:.1%} | é¢¨éšª: {risk_level}"
        )

        return {
            "overall_signal": overall_signal,
            "confidence": confidence,
            "strategy": strategy,
            "reasoning": reasoning,
            "risk_level": risk_level,
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        logger.error(f"ç”¢ç”Ÿæƒ…ç·’è¨Šè™Ÿå¤±æ•—: {e}", exc_info=True)
        return {
            "error": str(e),
            "overall_signal": "è§€æœ›",
            "confidence": 0.3,
            "strategy": "è§€æœ›",
            "reasoning": [f"è¨Šè™Ÿç”Ÿæˆå¤±æ•—: {str(e)}"],
            "risk_level": "é«˜",
            "timestamp": datetime.now().isoformat(),
        }


async def get_sentiment_agent(
    llm_model: LitellmModel = None,
    extra_headers: dict[str, str] = None,
    mcp_servers: list | None = None,
) -> Agent:
    """å‰µå»ºå¸‚å ´æƒ…ç·’åˆ†æ Agent

    Args:
        llm_model: ä½¿ç”¨çš„èªè¨€æ¨¡å‹å¯¦ä¾‹ (LitellmModel)ï¼Œå¦‚æœç‚º Noneï¼Œå‰‡ä½¿ç”¨é è¨­æ¨¡å‹
        extra_headers: é¡å¤–çš„ HTTP æ¨™é ­ï¼Œç”¨æ–¼æ¨¡å‹ API è«‹æ±‚
        mcp_servers: MCP servers å¯¦ä¾‹åˆ—è¡¨ï¼ˆMCPServerStdio å°è±¡ï¼‰ï¼Œå¾ TradingAgent å‚³å…¥

    Returns:
        Agent: é…ç½®å¥½çš„å¸‚å ´æƒ…ç·’åˆ†æ Agent

    Note:
        - ä¸ä½¿ç”¨ WebSearchTool å’Œ CodeInterpreterToolï¼ˆè¨—ç®¡å·¥å…·ä¸æ”¯æ´ ChatCompletions APIï¼‰
        - åªä½¿ç”¨è‡ªè¨‚å·¥å…·é€²è¡Œæƒ…ç·’åˆ†æ
        - Timeout ç”±ä¸» TradingAgent çš„ execution_timeout çµ±ä¸€æ§åˆ¶
        - Sub-agent ä½œç‚º Tool åŸ·è¡Œæ™‚æœƒå—åˆ°ä¸» Agent çš„ timeout é™åˆ¶
        - å·¥å…·æœƒè‡ªå‹•é€éå…¨å±€ä¸Šä¸‹æ–‡è¨ªå• tavily_mcp é€²è¡Œæ•¸æ“šè’é›†
    """
    logger.info(f"get_sentiment_agent() called with model={llm_model}")

    logger.debug("Creating custom tools with function_tool")

    # ç¢ºä¿ mcp_servers ç‚ºåˆ—è¡¨
    if mcp_servers is None:
        mcp_servers = []

    # æå– tavily_mcp ä¼ºæœå™¨ä¸¦è¨­ç½®åˆ°å…¨å±€ä¸Šä¸‹æ–‡ï¼ˆå·¥å…·å¯è¨ªå•ï¼‰
    if mcp_servers:
        for server in mcp_servers:
            if hasattr(server, "name") and server.name == "tavily_mcp":
                _sentiment_agent_context["tavily_mcp"] = server
                logger.debug("tavily_mcp å·²è¨­ç½®åˆ°å…¨å±€ä¸Šä¸‹æ–‡")
                break

    all_tools = [
        calculate_fear_greed_index,
        analyze_money_flow,
        analyze_news_sentiment,
        analyze_social_sentiment,
        generate_sentiment_signals,
    ]

    logger.debug(f"Total tools: {len(all_tools)}")
    tavily_available = _sentiment_agent_context.get("tavily_mcp") is not None
    logger.info(
        f"Creating Agent with model={llm_model}, mcp_servers={len(mcp_servers) if mcp_servers else 0}, "
        f"tools={len(all_tools)}, tavily_mcp={'available' if tavily_available else 'not available'}"
    )

    # GitHub Copilot ä¸æ”¯æ´ tool_choice åƒæ•¸
    model_settings_dict = {
        "max_completion_tokens": 500,  # æ§åˆ¶å›ç­”é•·åº¦ï¼Œé¿å…éåº¦å†—é•·
    }

    # åªæœ‰é GitHub Copilot æ¨¡å‹æ‰æ”¯æ´ tool_choice
    model_name = llm_model.model if llm_model else ""
    if "github_copilot" not in model_name.lower():
        model_settings_dict["tool_choice"] = "required"

    if extra_headers:
        model_settings_dict["extra_headers"] = extra_headers

    analyst = Agent(
        name="sentiment_analyst",
        instructions=sentiment_agent_instructions(),
        model=llm_model,
        mcp_servers=mcp_servers,
        tools=all_tools,
        model_settings=ModelSettings(**model_settings_dict),
    )
    logger.info("Sentiment Analyst Agent created successfully")

    return analyst
